//
//  Created by ktiays on 2025/2/12.
//  Copyright (c) 2025 ktiays. All rights reserved.
//

import Foundation

/// Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
///
/// https://platform.openai.com/docs/api-reference/chat/streaming
public struct ChatCompletionChunk: Decodable {
    /// A list of chat completion choices. Can contain more than one elements if
    /// ChatCompletionRequestBody's `n` property is greater than 1. Can also be empty for
    /// the last chunk, which contains usage information only.
    public internal(set) var choices: [Choice]

    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    public let created: Int?

    /// A unique identifier for the chat completion. Each chunk has the same ID.
    public let id: String?

    /// The model to generate the completion.
    public let model: String?

    /// The service tier used for processing the request.
    /// This field is only included if the `service_tier` parameter is specified in the request.
    public let serviceTier: String?

    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
    public let systemFingerprint: String?

    /// This property is nil for all chunks except for the last chunk, which contains the token
    /// usage statistics for the entire request.
    public let usage: ChatUsage?

    private enum CodingKeys: String, CodingKey {
        case choices
        case created
        case id
        case model
        case serviceTier = "service_tier"
        case systemFingerprint = "system_fingerprint"
        case usage
    }

    public init(
        choices: [Choice],
        created: Int? = nil,
        id: String? = nil,
        model: String? = nil,
        serviceTier: String? = nil,
        systemFingerprint: String? = nil,
        usage: ChatUsage? = nil
    ) {
        self.choices = choices
        self.created = created
        self.id = id
        self.model = model
        self.serviceTier = serviceTier
        self.systemFingerprint = systemFingerprint
        self.usage = usage
    }
}

public extension ChatCompletionChunk {
    /// https://platform.openai.com/docs/api-reference/chat/streaming#chat/streaming-choices
    struct Choice: Decodable {
        /// A chat completion delta generated by streamed model responses.
        public let delta: Delta

        /// The reason the model stopped generating tokens.
        ///
        /// This will be stop if the model hit a natural stop point or a provided stop sequence, length
        /// if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag
        /// from our content filters, or `tool_calls` if the model called a tool.
        public let finishReason: String?

        /// The index of the choice in the list of choices.
        public let index: Int?

        private enum CodingKeys: String, CodingKey {
            case delta
            case finishReason = "finish_reason"
            case index
        }

        public init(delta: Delta, finishReason: String? = nil, index: Int? = nil) {
            self.delta = delta
            self.finishReason = finishReason
            self.index = index
        }
    }
}

public extension ChatCompletionChunk.Choice {
    /// A chat completion delta generated by streamed model responses.
    struct Delta: Decodable {
        /// The contents of the chunk message.
        public let content: String?

        /// The reasoning content of the chunk message.
        public let reasoning: String?
        public let reasoningContent: String?

        /// The refusal message generated by the model.
        public let refusal: String?

        /// The role of the author of this message.
        public let role: String?

        public let toolCalls: [ToolCall]?

        private enum CodingKeys: String, CodingKey {
            case content
            case reasoning
            case reasoningContent = "reasoning_content"
            case refusal
            case role
            case toolCalls = "tool_calls"
        }

        public init(
            content: String? = nil,
            reasoning: String? = nil,
            reasoningContent: String? = nil,
            refusal: String? = nil,
            role: String? = nil,
            toolCalls: [ToolCall]? = nil
        ) {
            self.content = content
            self.reasoning = reasoning
            self.reasoningContent = reasoningContent
            self.refusal = refusal
            self.role = role
            self.toolCalls = toolCalls
        }
    }
}

public extension ChatCompletionChunk.Choice.Delta {
    struct ToolCall: Decodable {
        public let index: Int?

        /// The ID of the tool call.
        public let id: String?

        /// The type of the tool. Currently, only "function" is supported.
        public let type: String?

        /// The function to call
        public let function: Function?
    }
}

public extension ChatCompletionChunk.Choice.Delta.ToolCall {
    struct Function: Decodable {
        /// The name of the function to call.
        public let name: String?

        /// The arguments to call the function with, as generated by the model in JSON format.
        /// Note that the model does not always generate valid JSON, and may hallucinate parameters not
        /// defined by your function schema. Validate the arguments in your code before calling your function.
        public let arguments: String?
    }
}
